# ğŸŒŒ Antigravity : Local AI Integration

**Antigravity** est une extension VS Code puissante conÃ§ue pour intÃ©grer l'intelligence artificielle locale (via **Ollama**) ou distante directement dans votre flux de travail. DÃ©veloppez plus vite avec un contexte intelligent, des correctifs automatiques et une interface immersive.

---

## âœ¨ FonctionnalitÃ©s

- **ğŸ§  Contexte Intelligent** : L'IA comprend le fichier actif et peut mÃªme analyser les fichiers liÃ©s (imports) pour des rÃ©ponses ultra-pertinentes.
- **ğŸ› ï¸ Multi-File Patching** : Appliquez les suggestions de l'IA directement dans votre code avec une vue "Diff" pour valider les changements.
- **ğŸš€ Support Hybride** : Utilisez vos modÃ¨les locaux avec **Ollama** ou connectez des APIs externes (OpenAI, etc.) via vos propres clÃ©s.
- **ğŸ“ Outils Git intÃ©grÃ©s** : GÃ©nÃ©rez des messages de commit pertinents ou demandez une revue de votre `git diff` en un clic.
- **ğŸ¨ Interface "Black Hole"** : Une expÃ©rience utilisateur fluide et cinÃ©matique avec un thÃ¨me sombre premium.

---

## ğŸš€ Installation Rapide (via .vsix)

Pas besoin de compiler le code ! Suivez ces Ã©tapes simples :

1.  **TÃ©lÃ©charger le fichier** : Allez dans la section **Releases** (ou dans le dossier `bin/` du dÃ©pÃ´t) et tÃ©lÃ©chargez le fichier `local-ai-integration-0.0.1.vsix`.
2.  **Installer sur VS Code** :
    - Ouvrez VS Code.
    - Allez dans l'onglet **Extensions** (`Ctrl+Shift+X`).
    - Cliquez sur les trois petits points (**...**) en haut Ã  droite du menu des extensions.
    - Choisissez **"Installer Ã  partir de VSIX..."** (Install from VSIX...).
    - SÃ©lectionnez le fichier tÃ©lÃ©chargÃ©.
3.  **Lancer Ollama** : Assurez-vous qu'Ollama est bien lancÃ© sur votre machine ([ollama.com](https://ollama.com/)).

---

## ğŸ› ï¸ Configuration

Une fois installÃ©e, l'extension est prÃªte Ã  l'emploi. Voici comment la lier Ã  vos modÃ¨les :

1.  **Ouvrez la vue Antigravity** dans la barre latÃ©rale gauche.
2.  **SÃ©lection du modÃ¨le** : Utilisez le menu dÃ©roulant pour voir les modÃ¨les disponibles sur votre instance Ollama locale (ex: `llama3`, `mistral`, `codellama`).
3.  **Connexion Distante / Cloud** : Pour relier l'extension Ã  un serveur Ollama distant ou un service cloud :
    - Cliquez sur l'icÃ´ne ğŸ”‘ (**Configuration**) dans la barre latÃ©rale.
    - Choisissez l'option **"âš¡ Ollama"**.
    - Saisissez l'URL complÃ¨te de votre serveur (ex: `http://votre-ip-ou-domaine:11434`).
    - L'extension synchronisera immÃ©diatement les modÃ¨les disponibles sur ce serveur distant.
4.  **Gestion des clÃ©s API** : Pour les APIs compatibles (OpenAI, etc.), utilisez le mÃªme bouton ğŸ”‘ pour configurer l'URL du point d'entrÃ©e et votre clÃ© secrÃ¨te.

---

## ğŸ“– Comment l'utiliser ?

- **Chat contextuel** : Posez des questions sur votre code actuel. L'extension inclut automatiquement le fichier ouvert dans la conversation.
- **Bouton ğŸ”— LiÃ©s** : Cliquez dessus pour que l'IA lise aussi les fichiers que votre code importe (trÃ¨s utile pour comprendre les dÃ©pendances).
- **Bouton ğŸ§  RÃ©flexion** : Active un mode oÃ¹ l'IA prend le temps de planifier sa solution avant de proposer du code.
- **Bouton ğŸ’¾ Commit** : L'IA analyse vos changements non commitÃ©s et vous propose un message clair et structurÃ©.

---

## ğŸ¤ Contribution

Ce projet est Open Source. Si vous souhaitez modifier le code ou ajouter des fonctionnalitÃ©s :
1. Clonez le dÃ©pÃ´t.
2. `npm install`
3. `npm run compile`
4. Appuyez sur `F5` pour tester vos modifications dans une nouvelle fenÃªtre.

---
*DÃ©veloppÃ© avec passion pour rendre l'IA locale accessible Ã  tous les dÃ©veloppeurs.*